---
title: "IS624 Assignment3"
author: "James Quacinella"
date: "06/22/2015"
output: 
  pdf_document:
    toc: yes
theme: journal
---

```{r include=FALSE}
# Init
library(caret)
library(mlbench)
library(AppliedPredictiveModeling)

# Non-Linear Models
library(nnet)         # Neural Net Models
library(earth)        # MARS models
library(kernlab)      # Support vector machines

# Formatting options
library(knitr)
opts_chunk$set(tidy=TRUE)
opts_chunk$set(tidy.opts=list(width.cutoff=50))

# Random seed
set.seed(1)

# Data Imports
data(permeability)
```


# Question 7.2

## Question

Friedman (1991) introduced several benchmark data sets create by simulation. 
One of these simulations used the following nonlinear equation to create data:

$$y = 10sin(n x_{1} x_{2}) + 20(x_{3} - 0.5) 2 + 10x_{4} + 5x_{5} + N(0, alpha^2 )$$

where the x values are random variables uniformly distributed between [0, 1]
(there are also 5 other non-informative variables also created in the simula-
tion). The package mlbench contains a function called mlbench.friedman1 that
simulates these data:

```{r fig.width=9}
trainingData <- mlbench.friedman1(200, sd = 1)
## We convert the ' x ' data from a matrix to a data frame
## One reason is that this will give the columns names.
trainingData$x <- data.frame(trainingData$x)
## Look at the data using
featurePlot(trainingData$x, trainingData$y)
## or other methods.
## This creates a list with a vector ' y ' and a matrix
## of predictors ' x ' . Also simulate a large test set to
## estimate the true error rate with good precision:
testData <- mlbench.friedman1(5000, sd = 1)
testData$x <- data.frame(testData$x)
```

Tune several models on these data. For example:

```{r}  
knnModel <- train(x = trainingData$x, y = trainingData$y, method = "knn", preProc = c("center", "scale"), tuneLength = 10)

knnModel

knnPred <- predict(knnModel, newdata = testData$x)
## The function ' postResample ' can be used to get the test set
## perforamnce values
postResample(pred = knnPred, obs = testData$y)
```

\pagebreak 

## Answer 

A kNN model was generated for us above. We have three options: train a neural network, a MARS model, and a SVM model (with potentially different kernels). Lets first look at a neural net model.

### Neural Net Model

```{r}
# Using train() led to worse results, and I am not sure what the
# difference is in using train versus nnet 
#
#nnetGrid <- expand.grid(.decay = c(0.01), .size=c(5))
#nnetFit <- train(trainingData$x, trainingData$y,
#                method = "nnet",
#                linout = TRUE,
#                trace = FALSE,
#                maxit = 500,
#                tuneGrid = nnetGrid,
#                MaxNWts = 5 * (ncol(trainingData$x) + 1) + 5 + 1)

# Create a Neural Net model
nnetFit <- nnet(trainingData$x, trainingData$y, size = 5,
                decay = 0.01, linout = TRUE, trace = FALSE, maxit = 500,
                MaxNWts = 5 * (ncol(trainingData$x) + 1) + 5 + 1)

# Show the fit
nnetFit
summary(nnetFit)

# Predict on the test set and see how well it did
nnetPredict <- predict(nnetFit, testData$x)
postResample(pred = nnetPredict, obs = testData$y)
```

**Discussion:** This basic model ends up with a high $R^2$ with no tuning of any model parameters. However, I tried creating a model using tain() instead of nnet(), but the results seemed worse off for some reason. I am not sure what the difference is between train() and using the model directly.

\pagebreak

### MARS Model

```{r}
# Create a MARS model
marsFit <- earth(trainingData$x, trainingData$y)

# Show the fit
marsFit
summary(marsFit)

# Predict on the test set and see how well it did
marsPredict <- predict(marsFit, testData$x)
postResample(pred = marsPredict, obs = testData$y)

# Inspect the model's use of predictors
plotmo(marsFit, caption="Predictors vs Observed in Additive MARS Tuned Model")
```

**Discussion:** It does seem like the first 5 predictors are noted as important, and left the other predictors out. Pretty impressive there, MARS. Lets try what the book has for tuning this model:

```{r}
marsGrid <- expand.grid(.degree = 1:2, .nprune = 2:38)
marsTuned <- train(trainingData$x, trainingData$y, method = "earth",
                  tuneGrid = marsGrid, 
                  trControl = trainControl(method = "cv"))

marsTuned
#summary(marsTuned)

# Predict on the test set and see how well it did
marsPredictTuned <- predict(marsTuned, testData$x)
postResample(pred = marsPredictTuned, obs = testData$y)

# Inspect the model's use of predictors (notice this does not work here as
# I get an error about wrong types; assuming this only works for models 
# from using earth())
#plotmo(marsTuned, caption="Predictors vs Observed in Additive MARS Tuned Model")
```

This model does even better, with an $R^2$ of 0.935!

\pagebreak

### Support Vector Machine Model

```{r}
# Create a SVM model
svmFit <- ksvm(y ~ ., data=as.data.frame(trainingData),
                kernel="rbfdot", 
               kpar="automatic", C=1, epsilon=0.1)

# Show the fit
svmFit
summary(svmFit)

# Predict on the test set and see how well it did
svmPredict <- predict(svmFit, as.data.frame(testData))
postResample(pred = svmPredict, obs = testData$y)
```

Lets tune a hopefuly better SVM model:

```{r results='hold'}
# Create a tuned SVM model w/ a radial kernel
svmRTuned.radial <- train(trainingData$x, trainingData$y,
                        method = "svmRadial",
                        preProc = c("center", "scale"),
                        tuneLength = 14,
                        trControl = trainControl(method = "cv"))

# Show the fit
#svmRTuned.radial
#summary(svmRTuned.radial)

# Predict on the test set and see how well it did
svmTunedPredict.radial <- predict(svmRTuned.radial, testData$x)
print("SVM Model with Radial Kernel")
postResample(pred = svmTunedPredict.radial, obs = testData$y)



# Create a tuned SVM model w/ a polynomial kernel
svmRTuned.poly <- train(trainingData$x, trainingData$y,
                        method = "svmPoly",
                        preProc = c("center", "scale"),
                        tuneLength = 3,
                        trControl = trainControl(method = "cv"))

# Show the fit
#svmRTuned.poly
#summary(svmRTuned.poly)

# Predict on the test set and see how well it did
svmTunedPredict.poly <- predict(svmRTuned.poly, testData$x)
print("SVM Model with Poly Kernel")
postResample(pred = svmTunedPredict.poly, obs = testData$y)
```

An SVM model with a radial kernel works best with an $R^2$ of 0.84. The overall results show that the Nueral Bet model and the MARS model work the best with the models explaining 90%+ of the variance in the data.

\pagebreak

# Question 7.4

## Question

Return to the permeability problem outlined in Exercise 6.2. Train 
several nonlinear regression models and evaluate the resampling and test set
performance.

(a) Which nonlinear regression model gives the optimal resampling and test
set performance?

(b) Do any of the nonlinear models outperform the optimal linear model you
previously developed in Exercise 6.2? If so, what might this tell you about
the underlying relationship between the predictors and the response?

(c) Would you recommend any of the models you have developed to replace
the permeability laboratory experiment?

## Answer

Like above, we will try out four kinds of models, and we'll see which one does
the best and if these models are better than the linear models created in the previous 
assignment.

### Data Pre-Processing

Before creating models, we should make sure the data is properly pre-processed. I will follow the same exact moethodology as per the previous assignment:

```{r}
# Get rid of any predictors that are nero-zero variance
nearZero <- nearZeroVar(fingerprints)
fingerprints.filtered <- fingerprints[, -nearZero]

# Filter out highly correlated predictors
correlations <- cor(fingerprints.filtered)
highCorr <- findCorrelation(correlations, cutoff = .9)
fingerprints.filtered <- fingerprints.filtered[, -highCorr]

# Split the data into a training and test set
fingerprints.train <- fingerprints.filtered[1:124,]
fingerprints.test <- fingerprints.filtered[1:124,]
permeability.train <- permeability[1:124, ]
permeability.test <- permeability[1:124, ]
```

The three models will be presented next, with results to follow.

\pagebreak

### Neural Net Model

```{r}
```{r}
# Create a Neural Net model
nnetFit <- nnet(fingerprints.train, permeability.train,
                size = 5,
                decay = 0.01,
                linout = TRUE,
                trace = FALSE,
                maxit = 500,
                MaxNWts = 5 * (ncol(fingerprints.test) + 1) + 5 + 1)

# Show the fit
nnetFit
#summary(nnetFit)

# Predict on the test set and see how well it did
nnetPredict <- predict(nnetFit, fingerprints.test)
postResample(pred = nnetPredict, obs = permeability.test)
```

\pagebreak

### KNN Model

```{r}
# Create a KNN model
# Without trControl = trainControl(method = "cv"), this does even worse
knnFit <- train(x = fingerprints.train, y = permeability.train, method = "knn", tuneLength = 10, preProc = c("center", "scale"), tuneGrid = data.frame(.k=1:20), trControl = trainControl(method = "cv"))

# Show the model
knnFit

# Lets make predictions for the test set
knnPred <- predict(knnFit, newdata = fingerprints.test)
postResample(pred = knnPred, obs = permeability.test)
```

### MARS Model

```{r}
# Create a MARS model
marsFit <- earth(fingerprints.train, permeability.train)

# Show the fit
marsFit
#summary(marsFit)

# Predict on the test set and see how well it did
marsPredict <- predict(marsFit, fingerprints.test)
postResample(pred = marsPredict, obs = permeability.test)

# Inspect the model's use of predictors
plotmo(marsFit, caption="Predictors vs Observed in Additive MARS Model")
```

\pagebreak

### Support Vector Machine Model

```{r}
df.fingerprints.train <- as.data.frame(fingerprints.train)
df.fingerprints.train$y <- permeability.train
df.fingerprints.test <- as.data.frame(fingerprints.test)

svmFit <- ksvm(y ~ ., data=df.fingerprints.train,
                kernel="rbfdot", 
               kpar="automatic", C=1, epsilon=0.1)

svmFit
#summary(svmFit)

svmPredict <- predict(svmFit, df.fingerprints.test)
postResample(pred = svmPredict, obs = permeability.test)
```

Lets tune a hopefuly better SVM model:

```{r}
svmRTuned <- train(fingerprints.train, permeability.train,
                  method = "svmRadial",
                  preProc = c("center", "scale"),
                  tuneLength = 14,
                  trControl = trainControl(method = "cv"))

svmRTuned
#summary(svmRTuned)

svmTunedPredict <- predict(svmRTuned, fingerprints.test)
postResample(pred = svmTunedPredict, obs = permeability.test)
```

\pagebreak

## Results and Discussions

Lets comapre all the models in one spot:

```{r results='hold'}
print("Neural Net Performance")
postResample(pred = nnetPredict, obs = permeability.test)
print("KNN Performance")
postResample(pred = knnPred, obs = permeability.test)
print("MARS Performance")
postResample(pred = marsPredict, obs = permeability.test)
print("SVM Performance")
postResample(pred = svmPredict, obs = permeability.test)
print("SVM Tuned Performance")
postResample(pred = svmTunedPredict, obs = permeability.test)
```

It looks like the tuned SVM model the Neural Net model do really well and are the best of the non-linear models checked here.

In my previous assignment, I was able to get a linear model with an $R^2$ of .91, so I would only recommend the Neural Net model since it seems to have better performance. I might recommend the tuned SVM model as another potential contender because it has a similar $R^2$ to the best linear model. However, most other linear models had much worse results, and was only able to get near the non-linear models with regularization. This suggests that the data does have some non-linearity to it.