---
title: "IS624 - Final"
author: "James Quacinella"
date: "07/12/2015"
output: 
  pdf_document:
    toc: yes
theme: journal
---

```{r include=FALSE}
# Multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
```


## Introduction

To quote directly from the [data source itself](https://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant):

>The dataset contains 9568 data points collected from a Combined Cycle Power Plant over 6 years
>(2006-2011), when the power plant was set to work with full load. Features consist of hourly
>average ambient variables Temperature (T), Ambient Pressure (AP), Relative Humidity (RH) and
>Exhaust Vacuum (V) to predict the net hourly electrical energy output (EP)  of the plant.
>
>A combined cycle power plant (CCPP) is composed of gas turbines (GT), steam turbines (ST) and
>heat recovery steam generators. In a CCPP, the electricity is generated by gas and steam
>turbines, which are combined in one cycle, and is transferred from one turbine to another.
>While the Vacuum is collected from and has an effect on the Steam Turbine, the other three o
>the ambient variables effect the GT performance.

In this project, I will try to develop predictive models for this dataset. I will start with linear regression and see what other models can bring.



## Data Preperation

The first thing to do was convert the .ods file into a .csv file using LibreOffice. The data.csv file is the result, and has been loaded:

```{r results='hide', message=FALSE}
# Init
library(e1071)
library(caret)
library(corrplot)
library(ggplot2)
library(GGally)
library(forecast)

# Set random set for predictability
set.seed(200)

# Load data
df <- read.csv("CCPP/data.csv")
predictors <-  c("V", "AT", "RH", "AP")
#df.predictors <- df[ , c("V", "AT", "RH", "AP")]
#df.predict <- data.frame(PE=df[ , c("PE")])
```

Lets see if there are any non-complete cases in the data:

```{r results='hide'}
sum(complete.cases(df$AT)) == nrow(df) # True
sum(complete.cases(df$V)) == nrow(df)  # True
sum(complete.cases(df$AP)) == nrow(df) # True
sum(complete.cases(df$RH)) == nrow(df) # True
```

It looks like we have a full data set with no missing NA values. Lets see if any of the predictors are significantly skewed:

```{r}
skewValues <- apply(df[, predictors], 2, skewness)
head(skewValues)
```

From this I would say that there is no significant skewness here, so Box Cox transformations are not needed.

TODO: 
* should we transform via center and scaling?


Lets remove any near zero variance predictors:

```{r}
remove <- nearZeroVar(df[ , predictors])   # No columns are near zero variance
```

Are there any correlations between predictors?

```{r}
correlations <- cor(df[ , predictors])
corrplot::corrplot(correlations, order = "hclust")
```

From this we can see some correlations between predictors, but that might be normal: the operating conditions of the plant probably change according to fundamental physical laws and operating contraints, so their changes over time might be correlated.

Lets view that data:

```{r}
ggpairs(df[ , predictors])

p1 <- ggplot(df, aes(x=AT, y=PE)) + geom_point() + geom_smooth(method='lm') + ggtitle("Output versus Ambient Temperature") + ylab("Energy Output")
p2 <- ggplot(df, aes(x=AP, y=PE)) + geom_point() + geom_smooth(method='lm') + ggtitle("Output versus Ambient Pressure") + ylab("Energy Output")
p3 <- ggplot(df, aes(x=RH, y=PE)) + geom_point() + geom_smooth(method='lm') + ggtitle("Output versus Relative Humidity") + ylab("Energy Output")
p4 <- ggplot(df, aes(x=V, y=PE)) + geom_point() + geom_smooth(method='lm') + ggtitle("Output versus Exhaust Vacuum") + ylab("Energy Output")
multiplot(p1, p2, p3, p4, cols=2)
```

For the next steps, where we build models for this data, lets create a training and test set for performance measures:

```{r}
trainIndex <- createDataPartition(df$PE, p = .8, list=FALSE)
df.training <- df[trainIndex, ]
df.test <- df[-trainIndex, ]
```

## Linear Regression Model

```{r}
# Build Lineat Regression Model
fit  <- lm(PE ~ V + AT + RH + AP, data=df.training)
#summary(fit)

# Residuals
plot(residuals(fit) ~ PE, data=df.training, main="Residuals Plot")
abline(0, 0, col='red')

# Forecast the data
forecast <- forecast(fit, newdata=df.test)
accuracy(forecast, df.test)
postResample(pred = forecast$mean, obs = df.test$PE)

# Plot predictions versus reality in test set
plot(df.test$PE, forecast$mean, main="Predictions of Energy Output versus Test Set", ylab="Energy Output (Prediction)", xlab="Energy Output (Test Set)")
abline(0, 1, col='red')
```

## Linear Model with Less Predictors

Lets try taking out one of the highly correlated predictors and see if we can improve our model:

```{r}
# Build Linear Regression Model
fit  <- lm(PE ~ V + RH + AP, data=df.training)
#summary(fit)

# Residuals
#plot(residuals(fit) ~ PE, data=df.training, main="Residuals Plot")
#abline(0, 0, col='red')

# Forecast the data
forecast <- forecast(fit, newdata=df.test)
accuracy(forecast, df.test)
postResample(pred = forecast$mean, obs = df.test$PE)

# Plot predictions versus reality in test set
#plot(df.test$PE, forecast$mean, main="Predictions of Energy Output versus Test Set", ylab="Energy Output (Prediction)", xlab="Energy Output (Test Set)")
#abline(0, 1, col='red')
```

Taking out the Ambient Temperature predictor, despite having high correlations to the other predictors, does not seem to help here.

## More Advanced Models

## Results

## Citations

Pınar Tüfekci, Prediction of full load electrical power output of a base load operated combined cycle power plant using machine learning methods, International Journal of Electrical Power & Energy Systems, Volume 60, September 2014, Pages 126-140, ISSN 0142-0615, http://dx.doi.org/10.1016/j.ijepes.2014.02.027.
(http://www.sciencedirect.com/science/article/pii/S0142061514000908)

Heysem Kaya, Pınar Tüfekci , Sadık Fikret Gürgen: Local and Global Learning Methods for Predicting Power of a Combined Gas & Steam Turbine, Proceedings of the International Conference on Emerging Trends in Computer and Electronics Engineering ICETCEE 2012, pp. 13-18 (Mar. 2012, Dubai)